#!/bin/bash

IP=$(ip -o -4 addr list eth0 | perl -n -e 'if (m{inet\s([\d\.]+)\/\d+\s}xms) { print $1 }')
LOCAL_IP=localhost
LOCAL_PORT=7077

echo "SPARK_SINGLE_NODE_IP=$IP"

echo "preparing Spark"
# This is now unfortunately required, because shell needs to be executed in special scope to be able to update env vars
sed -i s/__SPARK_LOCAL_IP__/$LOCAL_IP/ /opt/spark-$SPARK_VERSION/conf/spark-env.sh
sed -i s/__MASTER__/$LOCAL_IP/ /opt/spark-$SPARK_VERSION/conf/spark-env.sh
. /opt/spark-1.2.0/conf/spark-env.sh

env

echo "starting Hadoop Namenode"
hadoop namenode -format > /dev/null 2>&1
service hadoop-namenode start > /dev/null 2>&1
service hadoop-datanode start

echo "starting sshd"
/usr/sbin/sshd

sleep 5

echo "starting Spark Master at spark://$LOCAL_IP:$LOCAL_PORT"
nohup ${SPARK_HOME}/sbin/start-master.sh &

echo "starting Spark Worker at $LOCAL_IP"
nohup ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker spark://$LOCAL_IP:$LOCAL_PORT -h $LOCAL_IP < /dev/null 2>&1 > /dev/null &
echo "worker started"

echo "starting driver"
nohup java -jar /app/spark-assembly-1.0.0-SNAPSHOT.jar &

while [ 1 ];
do
	tail -f /opt/spark-${SPARK_VERSION}/logs/*.out
        sleep 1
done